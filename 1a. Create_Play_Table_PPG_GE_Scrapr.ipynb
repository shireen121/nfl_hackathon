{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose of this script:\n",
    "\n",
    "Extract relevant play data fields from multiple tables provided including plays_playlist_game, game_events and NFL_scrapr to create a master play level table with key attributes for modeling and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import s3fs\n",
    "import re\n",
    "from datetime import datetime\n",
    "# import sys\n",
    "# import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plays Playlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to download and concatenate all raw data from the plays_playlist_game S3 bucket for a given year\n",
    "def ppg_consolidator(year):\n",
    "    fs = s3fs.S3FileSystem()\n",
    "    s3_path='nyg-hackathon-154843675742/source_file/nfl/plays_playlist_game/year={}/'.format(year)\n",
    "\n",
    "    # list of all s3 file paths\n",
    "    games_plays_filenames = fs.ls('s3://{}/'.format(s3_path))\n",
    "    print('Number of PPG files for {} : '.format(year), len(games_plays_filenames))\n",
    "\n",
    "    #read in all files and concatenate into one Dataframe\n",
    "    s3 = boto3.resource('s3')\n",
    "    plays = []\n",
    "    for file in games_plays_filenames:\n",
    "        split_filename = file.split('/',maxsplit=1)\n",
    "        bucket = split_filename[0]\n",
    "        datakey = split_filename[1]\n",
    "        obj= s3.Object(bucket, datakey)\n",
    "        body = obj.get()['Body'].read().decode('utf-8')\n",
    "        data = json.loads(body)\n",
    "        temp = pd.DataFrame(data['plays'])\n",
    "        plays.append(temp)\n",
    "    df_ppg = pd.concat(plays).reset_index()\n",
    "    df_ppg = df_ppg.drop('index', axis=1)\n",
    "    \n",
    "    return df_ppg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to parse and cleanse plays_playlist_game data\n",
    "def clean_ppg_table(df):\n",
    "\n",
    "    #prep sorted dataset for parsing\n",
    "    dfdown = df.loc[df.down > 0] # Removes kickoffs, possibly more (?)\n",
    "    dfd_sorted = dfdown.sort_values(by=\"sequence\")\n",
    "    \n",
    "    # parses offense and defense columns \n",
    "    # note: punts and non-snap plays (and possibly more) don't have the same formation info\n",
    "\n",
    "    dfd_trans = dfd_sorted.copy()\n",
    "    parser = lambda xy: [(k.get(xy[1]) if type(k).__name__ == 'dict' else None) for k in dfd_sorted[xy[0]]]\n",
    "\n",
    "    dfd_trans['oForm'] = parser(('offense','offenseFormation'))\n",
    "    dfd_trans['oPersonnel'] = parser(('offense','personnel'))\n",
    "    dfd_trans['dInBox'] = parser(('defense','defendersInTheBox'))\n",
    "    dfd_trans['dRushers'] = parser(('defense', 'numberOfPassRushers'))\n",
    "    dfd_trans['dPersonnel'] = parser(('defense', 'personnel'))\n",
    "    dfd_trans['allPersonnel'] = dfd_trans['oPersonnel'] + ', ' + dfd_trans['dPersonnel']\n",
    "    \n",
    "    # 'missing' values show as NaN\n",
    "\n",
    "    dfd_trans.drop(['offense', 'defense', 'oPersonnel', 'dPersonnel'], axis=1, inplace=True)\n",
    "    \n",
    "    #extract number of players in each position\n",
    "    def research(pos, allpos):\n",
    "        try:\n",
    "            return (re.search(\"\\\\d(?=\\\\s\"+pos+\")\", allpos).group(0))\n",
    "        except:\n",
    "            return None\n",
    "    for pos in ['RB', 'TE', 'WR', 'DL', 'LB', 'DB']:\n",
    "        dfd_trans[pos] = [research(pos, k) for k in dfd_trans['allPersonnel']]\n",
    "    dfd_trans.drop('allPersonnel', axis=1, inplace=True)\n",
    "    \n",
    "    #parse play description field between playtime and description\n",
    "    dfd_trans['playDesc']=[re.sub('\\\\(.*?\\\\)\\\\s','',x,count=1) for x in dfd_trans['playDescription']]\n",
    "    dfd_trans['playTime']=[re.search('(?<=\\\\().*?(?=\\\\)\\\\s)',x).group(0) for x in dfd_trans['playDescription']]\n",
    "    \n",
    "  \n",
    "    #filter for only regular season games\n",
    "    dfd_trans = dfd_trans[dfd_trans['seasonType'] == 'REG'].copy()\n",
    "    \n",
    "    #keep relevant columns in final df to export\n",
    "    ppg_keep = dfd_trans[['gameId', 'playId', 'homeScore', 'visitorScore', 'possessionTeam', 'down',\n",
    "                          'yardsToGo', 'isRedzonePlay', 'isScoring', 'yardlineNumber', 'yardlineSide', 'oForm',\n",
    "                          'dInBox', 'dRushers', 'RB', 'TE', 'WR', 'DL', 'LB', 'DB', 'playDesc']]\n",
    "    \n",
    "    \n",
    "    return ppg_keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to download and concatenate all raw data from the game_events S3 bucket for a given year\n",
    "def ge_consolidator(year):\n",
    "    fs = s3fs.S3FileSystem()\n",
    "    s3_path='nyg-hackathon-154843675742/source_file/nfl/game_events/year={}/'.format(year)\n",
    "\n",
    "    # list of all s3 file paths\n",
    "    games_events_filenames = fs.ls('s3://{}/'.format(s3_path))\n",
    "    print('Number of GE files for {} : '.format(year), len(games_events_filenames))\n",
    "\n",
    "    #read in all files and concatenate into one Dataframe\n",
    "    s3 = boto3.resource('s3')\n",
    "    events = []\n",
    "    for file in games_events_filenames:\n",
    "        split_filename = file.split('/',maxsplit=1)\n",
    "        bucket = split_filename[0]\n",
    "        datakey = split_filename[1]\n",
    "        obj= s3.Object(bucket, datakey)\n",
    "        body = obj.get()['Body'].read().decode('utf-8')\n",
    "        data = json.loads(body)\n",
    "        temp = pd.DataFrame(data['events'])\n",
    "        events.append(temp)\n",
    "    df_ge = pd.concat(events).reset_index()\n",
    "    df_ge = df_ge.drop('index', axis=1)\n",
    "    \n",
    "    return df_ge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to parse and cleanse game_events data\n",
    "def clean_ge_table(df):\n",
    "    \n",
    "    #prep sorted dataset for parsing\n",
    "    dfedown = df.loc[df.down > 0] \n",
    "    dfe_sorted = dfedown.sort_values(by=\"sequence\")\n",
    "\n",
    "    # Drop columns already in PPG (don't drop join cols)\n",
    "    dfe_sorted.drop(['down', 'isSTPlay', 'isScoring', 'playDescription', 'sequence', 'yardsToGo',\n",
    "                    'possessionTeamId', 'yardline', 'yardlineNumber', 'yardlineSide'], axis=1, inplace=True)\n",
    "    \n",
    "    #extract start and endtime in datetime format\n",
    "    def datetry(x):\n",
    "        try:\n",
    "            return(datetime.strptime(x[:-1], \"%Y-%m-%dT%H:%M:%S.%f\"))\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    dfe_sorted['startTime'] = [datetry(x) for x in dfe_sorted.startPlayTime]\n",
    "    dfe_sorted['endTime'] = [datetry(x) for x in dfe_sorted.endPlayTime]\n",
    "\n",
    "    dfe_sorted.drop(['startPlayTime', 'endPlayTime'], axis=1, inplace=True)\n",
    "\n",
    "    #extract snaps\n",
    "    def getsnap(eventset):\n",
    "        y = [k['time'] if k['name']=='ball_snap' else None for k in eventset]\n",
    "        z = list(filter(None.__ne__, y))\n",
    "        return (z[0] if len(z) > 0 else None)\n",
    "    \n",
    "    dfe_sorted['snap'] = [getsnap(x) if type(x) != float else None for x in dfe_sorted.events]\n",
    "    dfe_snaps = dfe_sorted.loc[pd.notnull(dfe_sorted['snap'])]\n",
    "    dfe_snaps['snapTime'] = [datetry(x) for x in dfe_snaps['snap']]\n",
    "    del dfe_snaps['snap']\n",
    "\n",
    "    # Drop columns we're not keeping for now (including the events, which we can parse and curate later)\n",
    "    dfe_snaps.drop(['timeOfDayUTC', 'events'], axis=1, inplace=True)\n",
    "    return dfe_snaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funtion to merge playlist_plays and game_events data\n",
    "def combine_ppg_ge(year):\n",
    "    ppg_df = clean_ppg_table(ppg_consolidator(2019))\n",
    "    ge_df = clean_ge_table(ge_consolidator(2019))\n",
    "    \n",
    "    df_comb = pd.merge(left=ppg_df, right=ge_df, on=[\"gameId\", \"playId\"], how=\"left\")\n",
    "  \n",
    "\n",
    "    df_final = df_comb[['gameId','playId','homeScore','visitorScore','quarter','gameClock', \n",
    "                        'startTime', 'endTime', 'snapTime', 'possessionTeam','isGoalToGo','yardlineNumber','yardlineSide',\n",
    "                       'absoluteYardlineNumber','isRedzonePlay','isScoring','isPenalty',\n",
    "                        'oForm','dInBox','RB','TE','WR','DL','LB','DB',\n",
    "                        'dRushers', 'playDesc' ]]\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of PPG files for 2019 :  332\n",
      "Number of GE files for 2019 :  333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "#create clean merged 2019 PPG and GE table\n",
    "ppg_ge_19 = combine_ppg_ge(2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NFLScrapr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load regular season games\n",
    "s3b = 'nyg-hackathon-154843675742'\n",
    "data_key = 'source_file/nfl/nflscrapr_data/season=regular/reg_pbp_2017.csv'\n",
    "data_location = 's3://{}/{}'.format(s3b, data_key)\n",
    "\n",
    "scrapr_2017_reg = pd.read_csv(data_location, dtype = str)\n",
    "\n",
    "\n",
    "data_key = 'source_file/nfl/nflscrapr_data/season=regular/reg_pbp_2018.csv'\n",
    "data_location = 's3://{}/{}'.format(s3b, data_key)\n",
    "\n",
    "scrapr_2018_reg = pd.read_csv(data_location, dtype = str)\n",
    "\n",
    "\n",
    "data_key = 'source_file/nfl/nflscrapr_data/season=regular/reg_pbp_2019.csv'\n",
    "data_location = 's3://{}/{}'.format(s3b, data_key)\n",
    "\n",
    "scrapr_2019_reg = pd.read_csv(data_location, dtype = str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapr_include = ['play_id', 'game_id', 'home_team', 'away_team', 'down', 'ydstogo', 'play_type', 'shotgun', 'no_huddle',\n",
    " 'qb_dropback', 'qb_scramble', 'pass_location', 'air_yards', 'run_location', 'run_gap', 'ep', 'epa', 'wp',\n",
    " 'home_wp', 'wpa', 'first_down_rush', 'first_down_pass', 'third_down_converted', 'third_down_failed', \n",
    " 'fourth_down_converted', 'fourth_down_failed', 'incomplete_pass', 'interception', 'fumble_forced', 'tackled_for_loss', \n",
    "'fumble_lost', 'qb_hit', 'rush_attempt', 'pass_attempt', 'sack', 'fumble', 'complete_pass', 'tackle_for_loss_1_player_id', \n",
    " 'tackle_for_loss_1_player_name', 'qb_hit_1_player_id', 'qb_hit_1_player_name', 'qb_hit_2_player_id', \n",
    "'qb_hit_2_player_name', 'forced_fumble_player_1_player_id', 'forced_fumble_player_1_player_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapr_2017_reg_short = scrapr_2017_reg[scrapr_include]\n",
    "scrapr_2018_reg_short = scrapr_2018_reg[scrapr_include]\n",
    "scrapr_2019_reg_short = scrapr_2019_reg[scrapr_include]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapr = pd.concat([scrapr_2017_reg_short, scrapr_2018_reg_short, scrapr_2019_reg_short])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge with ppg_ge table\n",
    "scrapr.rename(columns = {'game_id': 'gameId', 'play_id': 'playId'}, inplace=True)\n",
    "scrapr['playId'] = scrapr['playId'].astype(int)\n",
    "scrapr['gameId'] = scrapr['gameId'].astype(int)\n",
    "merged_df = ppg_ge_19.merge(scrapr, on=['gameId', 'playId'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reorder columns\n",
    "col_order = ['gameId', 'playId', 'home_team', 'away_team', 'homeScore', 'visitorScore', 'quarter', 'gameClock',\n",
    "       'startTime', 'endTime', 'snapTime',  'possessionTeam', 'playDesc', \n",
    "       'down', 'ydstogo', 'play_type',\n",
    "       'isGoalToGo', 'yardlineNumber', 'yardlineSide',\n",
    "       'absoluteYardlineNumber', 'isRedzonePlay', 'isScoring', 'isPenalty',\n",
    "       'oForm', 'dInBox', 'RB', 'TE', 'WR', 'DL', 'LB', 'DB', 'dRushers',\n",
    "        'shotgun', 'no_huddle', 'qb_dropback',\n",
    "       'qb_scramble', 'pass_location', 'air_yards', 'run_location', 'run_gap',\n",
    "       'ep', 'epa', 'wp', 'home_wp', 'wpa', 'first_down_rush',\n",
    "       'first_down_pass', 'third_down_converted', 'third_down_failed',\n",
    "       'fourth_down_converted', 'fourth_down_failed', 'incomplete_pass',\n",
    "       'interception', 'fumble_forced', 'tackled_for_loss', 'fumble_lost',\n",
    "       'qb_hit', 'rush_attempt', 'pass_attempt', 'sack', 'fumble',\n",
    "       'complete_pass', 'tackle_for_loss_1_player_id',\n",
    "       'tackle_for_loss_1_player_name', 'qb_hit_1_player_id',\n",
    "       'qb_hit_1_player_name', 'qb_hit_2_player_id', 'qb_hit_2_player_name',\n",
    "       'forced_fumble_player_1_player_id',\n",
    "       'forced_fumble_player_1_player_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export to csv - change version # if code updated and need to recraete dataframe\n",
    "# merged_df[col_order].to_csv('../data/master_play_table_v1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
