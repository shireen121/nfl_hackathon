{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose of this script:\n",
    "\n",
    "Extract player tracking data from the tracking game range dataset to create a master table that has a unique row for each player who is on the field for each play for a regular season game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import s3fs\n",
    "import re\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "# import pyarrow\n",
    "# from pyathena import connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in tracking game range playID table previously extracted from Athena\n",
    "tracking_playid = pd.read_csv('tracking_game_range_playid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to download raw data from the tracking_game_range S3 bucket for a given year\n",
    "\n",
    "def get_tracking_game_filenames(year):\n",
    "    fs = s3fs.S3FileSystem()\n",
    "    s3_path='nyg-hackathon-154843675742/source_file/nfl/tracking_game_range/year={}/'.format(year)\n",
    "    # list of all s3 file paths\n",
    "    tracking_data_filenames = fs.ls('s3://{}/'.format(s3_path))\n",
    "    print('Number of Tracking Game Range files for {} : '.format(year), len(tracking_data_filenames))\n",
    "    return tracking_data_filenames\n",
    "\n",
    "def load_tracking_game_data(file):\n",
    "    s3 = boto3.resource('s3')\n",
    "    split_filename = file.split('/',maxsplit=1)\n",
    "    bucket = split_filename[0]\n",
    "    datakey = split_filename[1]\n",
    "    obj= s3.Object(bucket, datakey)\n",
    "    body = obj.get()['Body'].read().decode('utf-8')\n",
    "    data = json.loads(body)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ppg_ge tables to get regular season gameIds\n",
    "play_master_df = pd.read_csv('../common/data/master_play_table_v1.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull gameIds for only regular season games\n",
    "reg_season_gameIds = sorted([str(x) for x in play_master_df.gameId.unique()])\n",
    "start_idx = len('nyg-hackathon-154843675742/source_file/nfl/tracking_game_range/year=2019/tracking_game_range_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tracking Game Range files for 2019 :  75369\n",
      "number of regular season files: 57290\n"
     ]
    }
   ],
   "source": [
    "#extract all regular season filenames\n",
    "player_dfs = []\n",
    "filenames = get_tracking_game_filenames(2019)\n",
    "reg_filenames = sorted([x for x in filenames if x[start_idx:start_idx+10] in reg_season_gameIds])\n",
    "\n",
    "print('Number of regular season files: {}'.format(len(reg_filenames)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tracking_data(df):\n",
    "    df['gameId'] = data['gameId']\n",
    "    df['playId'] = playid\n",
    "    \n",
    "    \n",
    "    #extract time slices, x and y coordinates from playerTrackingdata\n",
    "    df['coordinates'] = df.apply(lambda row: [[dict['time'], dict['x'], dict['y']] for dict in row.playerTrackingData], axis = 1)\n",
    "    df['isOnField'] = df.apply(lambda row: [dict['isOnField'] for dict in row.playerTrackingData], axis = 1)\n",
    "\n",
    "    #get start and times from play master df which holds the start (Snap) time and end of the play\n",
    "    play_times = play_master_df[['gameId', 'playId','snapTime', 'endTime']].copy()\n",
    "    play_times.rename(columns={'endTime': 'play_endTime'}, inplace=True)\n",
    "\n",
    "    #merge snaptime and play end time with player_df\n",
    "    df['gameId'] = df['gameId'].astype(int)\n",
    "    df['startTime'] = data['startTime']\n",
    "    df['endTime'] = data['endTime']\n",
    "    df['playId'] = df['playId'].astype(int)\n",
    "    df = df.merge(play_times, on=['gameId', 'playId'], how='left')\n",
    "    return df\n",
    "\n",
    "\n",
    "def group_data_by_play(df):\n",
    "    #rollup sequences to unique plays by player\n",
    "    static_cols = ['nflId', 'displayName', 'position', 'positionGroup', 'gameId', 'playId', 'snapTime', 'teamType', 'play_endTime']\n",
    "    df_groupbyplay = df.groupby(static_cols)[['coordinates', 'isOnField']].sum().reset_index()\n",
    "    \n",
    "    #subset tracking data to on field players and tracking during play time + 2 sec prior and 1 sec post play\n",
    "    df_groupbyplay['play_startIdx'] = df_groupbyplay.apply(lambda row: math.ceil(((datetime.strptime(row.snapTime, '%Y-%m-%d %H:%M:%S.%f') - datetime.strptime(row.coordinates[0][0], '%Y-%m-%dT%H:%M:%S.%f')).total_seconds()) * 10) - 20, axis = 1)\n",
    "    df_groupbyplay['play_endIdx'] = df_groupbyplay.apply(lambda row: math.ceil((datetime.strptime(row.play_endTime, '%Y-%m-%d %H:%M:%S.%f') - datetime.strptime(row.coordinates[0][0], '%Y-%m-%dT%H:%M:%S.%f')).total_seconds() * 10) + 10, axis = 1)\n",
    "    df_groupbyplay['isOnField_atSnap'] = df_groupbyplay.apply(lambda row: row['isOnField'][row.play_startIdx + 20] if (len(row.isOnField) > (row.play_startIdx + 20)) & (row.play_startIdx > 0) else False, axis=1)\n",
    "    df_groupbyplay = df_groupbyplay[df_groupbyplay['isOnField_atSnap'] == True]\n",
    "    df_groupbyplay.drop('isOnField', axis=1, inplace=True)\n",
    "    df_groupbyplay['coordinates'] = df_groupbyplay.apply(lambda row: [list[1:3] for list in row.coordinates[row.play_startIdx:row.play_endIdx]], axis = 1)\n",
    "    return df_groupbyplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251 games processed at 2020-07-21 13:21:22.118388\n",
      "252 games processed at 2020-07-21 13:23:07.569946\n",
      "253 games processed at 2020-07-21 13:24:50.547229\n",
      "254 games processed at 2020-07-21 13:26:15.318559\n",
      "255 games processed at 2020-07-21 13:27:48.162314\n",
      "256 games processed at 2020-07-21 13:29:11.342290\n"
     ]
    }
   ],
   "source": [
    "#extract all player tracking data for regular season 2019 - this takes hours ~5-6 hrs to run - RUN SPARINGLY\n",
    "master_player_df = pd.DataFrame()\n",
    "y=0\n",
    "for game in reg_season_gameIds:\n",
    "    #loop through each file and pull out relevant data for processing\n",
    "    filesforsinglegame = [x for x in reg_filenames if x[start_idx:start_idx+10] == game]\n",
    "    game_df = pd.DataFrame()\n",
    "    for file in filesforsinglegame:\n",
    "\n",
    "        data = load_tracking_game_data(file)\n",
    "        if data['awayTrackingData'] != [] and data['homeTrackingData'] != []:\n",
    "            \n",
    "            try: \n",
    "                #extract playid from tracking_game_range_playid table\n",
    "                playid = str(int(tracking_playid.loc[(tracking_playid['gameid'] == data['gameId']) & \n",
    "                        (tracking_playid['starttime'] == str(data['startTime'])) &\n",
    "                        (tracking_playid['endtime'] == str(data['endTime'])), 'playid']))\n",
    "                \n",
    "                #process tracking data from away and home tracking data columns\n",
    "                away_tracking = pd.DataFrame(data['awayTrackingData'])\n",
    "                away_tracking_proc = extract_tracking_data(away_tracking)\n",
    "                away_tracking_proc['teamType'] = 'away'\n",
    "                game_df = game_df.append(away_tracking_proc, ignore_index=True)\n",
    "\n",
    "                home_tracking = pd.DataFrame(data['homeTrackingData'])\n",
    "                home_tracking_proc = extract_tracking_data(home_tracking)\n",
    "                home_tracking_proc['teamType'] = 'home'\n",
    "                game_df = game_df.append(home_tracking_proc, ignore_index=True)\n",
    "                \n",
    "            except:\n",
    "                print('Oops!', sys.exc_info()[0], 'occurred for a play in game {}'.format(y+1))\n",
    "                pass\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "    y += 1\n",
    "    #roll up data to play by player level and process tracking data accordingly\n",
    "    df_groupbyplay = group_data_by_play(game_df)\n",
    "    print('{} games processed at {}'.format(y, datetime.now()))\n",
    "    master_player_df = master_player_df.append(df_groupbyplay)\n",
    "    if y % 50 == 0:\n",
    "        #save files incrementally to avoid losing progress \n",
    "        master_player_df.to_pickle('S3://nyg-hackathon-154843675742/halloffame/snap_player_dataframes/snap_player_df_{}.pkl'.format(str(y)))\n",
    "        print('Saved dataframe with {} games to S3 bucket'.format(y))\n",
    "        master_player_df = pd.DataFrame()\n",
    "\n",
    "#save final file \n",
    "master_player_df.to_pickle('S3://nyg-hackathon-154843675742/halloffame/snap_player_dataframes/snap_player_df_{}.pkl'.format(str(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in one file at a time otherwise kernel may die\n",
    "df_1 = pd.read_pickle('S3://nyg-hackathon-154843675742/halloffame/snap_player_dataframes/snap_player_df_50.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_pickle('S3://nyg-hackathon-154843675742/halloffame/snap_player_dataframes/snap_player_df_100.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = pd.read_pickle('S3://nyg-hackathon-154843675742/halloffame/snap_player_dataframes/snap_player_df_150.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4 = pd.read_pickle('S3://nyg-hackathon-154843675742/halloffame/snap_player_dataframes/snap_player_df_200.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5 = pd.read_pickle('S3://nyg-hackathon-154843675742/halloffame/snap_player_dataframes/snap_player_df_250.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_6 = pd.read_pickle('S3://nyg-hackathon-154843675742/halloffame/snap_player_dataframes/snap_player_df_256.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([df_1, df_2, df_3, df_4, df_5, df_6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined['coordinates'] = df_combined.coordinates.apply(lambda x: np.array(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract data\n",
    "# df_combined.to_pickle('../common/data/master_player_df.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
